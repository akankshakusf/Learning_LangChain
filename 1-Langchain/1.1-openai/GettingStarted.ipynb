{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "#initialize the env\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "#langsmith tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load gpt-4o model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x0000018C9DB63460> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000018C9DB894B0> root_client=<openai.OpenAI object at 0x0000018C9DB61600> root_async_client=<openai.AsyncOpenAI object at 0x0000018C9DB634C0> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "#load chat openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "#instantiate the gpt 4o model\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets ask some question the gpt 4o model\n",
    "result=llm.invoke(\"what is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI refers to a category of artificial intelligence systems designed to create new content, such as text, images, music, or other media, that is similar to human-generated content. These systems use machine learning models, often neural networks, to analyze patterns in existing data and generate novel outputs that mimic those patterns.\n",
      "\n",
      "Key types of generative AI include:\n",
      "\n",
      "1. **Generative Adversarial Networks (GANs):** These involve two neural networks—the generator and the discriminator—which are trained together. The generator creates new content, while the discriminator evaluates it against real-world data. They improve through this adversarial process, leading to the generation of increasingly realistic content.\n",
      "\n",
      "2. **Variational Autoencoders (VAEs):** These models encode input data into a lower-dimensional representation and then decode it back to the original space, allowing them to generate new data variations by sampling from the learned data distribution.\n",
      "\n",
      "3. **Transformer-based Models:** These include models like GPT (Generative Pre-trained Transformer) used for text generation. They leverage the transformer architecture to predict subsequent text segments, enabling them to generate coherent and contextually relevant text.\n",
      "\n",
      "Applications of generative AI span numerous fields, including art and design, entertainment, pharmaceuticals (for drug discovery), and chatbots, where it can produce engaging content, aid in creative processes, or simulate realistic interactions.\n"
     ]
    }
   ],
   "source": [
    "print((result).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chat prompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "#initalize the template\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langsmith is a comprehensive tool developed by the creators of LangChain to enhance the process of building, evaluating, and deploying applications that utilize large language models (LLMs). Key features of Langsmith include:\\n\\n1. **Logging and Monitoring**: Langsmith offers sophisticated logging capabilities that help developers observe and track how LLMs interact within applications. By monitoring the inputs and outputs, developers can better understand model behavior and performance.\\n\\n2. **Evaluation**: The tool provides functionalities to evaluate LLMs systematically. This could involve testing various model configurations, tracking performance metrics, and gaining insights into model efficacy across different use cases.\\n\\n3. **Fine-tuning and Optimization**: Although not directly related to fine-tuning models, Langsmith aids in experimenting with chaining different components, providing a structured environment to iterate and optimize workflows.\\n\\n4. **Deployment**: Langsmith supports deploying LLM-based applications, ensuring that the transition from development to production is smooth and efficient.\\n\\nOverall, Langsmith is built to streamline the iterative process of developing and deploying applications that rely on large language models, making it easier to manage complex interactions and leverage LLMs effectively across various tasks.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 234, 'prompt_tokens': 32, 'total_tokens': 266, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'stop', 'logprobs': None}, id='run-b8212f15-b46a-48fb-b9df-c25fd46a6771-0', usage_metadata={'input_tokens': 32, 'output_tokens': 234, 'total_tokens': 266, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now prepare a chain between llm model and prompt\n",
    "chain=prompt|llm\n",
    "\n",
    "#initailise the chain\n",
    "response=chain.invoke({\"input\":\"Can you tell me about langsmith\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make output clearer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a tool designed to enhance the development process of applications using large language models (LLMs). It offers advanced capabilities for testing, evaluating, and monitoring LLM applications, which helps streamline their creation and refinement. LangSmith integrates seamlessly with the LangChain framework, which is a popular toolkit for developing with LLMs, particularly in complex and dynamic applications. By providing these capabilities, LangSmith aims to improve the efficiency, reliability, and performance of LLM-based systems, making it easier for developers to harness the full potential of language models in their applications.\n"
     ]
    }
   ],
   "source": [
    "#import stroutput parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "#intialize the parser\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "#make the chain \n",
    "chain = prompt|llm|output_parser\n",
    "\n",
    "#ask for a response\n",
    "response=chain.invoke({\"input\":\"Can you tell me about langsmith\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
